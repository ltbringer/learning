# Interpretable AI

[book](https://www.manning.com/books/interpretable-ai)
 
TOC-wise progress

- [ ] 1 INTRODUCTION
    - [ ] 1.1 Diagnostics+ AI – An Example AI System
    - [ ] 1.2 Types of Machine Learning Systems
    - [ ] 1.2.1 Representation of Data
    - [ ] 1.2.2 Supervised Learning
    - [ ] 1.2.3 Unsupervised Learning
    - [ ] 1.2.4 Reinforcement Learning
    - [ ] 1.2.5 Machine Learning System for Diagnostics+ AI
    - [ ] 1.3 Building Diagnostics+ AI
    - [ ] 1.4 Gaps in Diagnostics+ AI
    - [ ] 1.4.1 Data Leakage
    - [ ] 1.4.2 Bias
    - [ ] 1.4.3 Regulatory Non-Compliance
    - [ ] 1.4.4 Concept Drift
    - [ ] 1.5 Building a Robust Diagnostics+ AI
    - [ ] 1.6 Interpretability v/s Explainability
    - [ ] 1.6.1 Types of Interpretability Techniques
    - [ ] 1.7 What will I learn in this book?
    - [ ] 1.7.1 What tools will I be using in this book?
    - [ ] 1.7.2 What do I need to know before reading this book?
    - [ ] 1.8 Summary
 
- [ ] 2 WHITE-BOX MODELS
    - [ ] 2.1 White-Box Models
    - [ ] 2.1.1 Diagnostics+ AI – Diabetes Progression
    - [ ] 2.2 Linear Regression
    - [ ] 2.2.1 Interpreting Linear Regression
    - [ ] 2.2.2 Limitations of Linear Regression
    - [ ] 2.3 Decision Trees
    - [ ] 2.3.1 Interpreting Decision Trees
    - [ ] 2.3.2 Limitations of Decision Trees
    - [ ] 2.4 Generalized Additive Models (GAMs)
    - [ ] 2.4.1 Regression Splines
    - [ ] 2.4.2 GAM for Diagnostics+ Diabetes
    - [ ] 2.4.3 Interpreting GAMs
    - [ ] 2.4.4 Limitations of GAMs
    - [ ] 2.5 Looking Ahead to Black-Box Models
    - [ ] 2.6 Summary
 
- [ ] 3 MODEL AGNOSTIC METHODS - GLOBAL INTERPRETABILITY
    - [ ] 3.1 High School Student Performance Predictor
    - [ ] 3.1.1 Exploratory Data Analysis
    - [ ] 3.2 Tree Ensembles
    - [ ] 3.2.1 Training a Random Forest
    - [ ] 3.3 Interpreting a Random Forest
    - [ ] 3.4 Model Agnostic Methods – Global Interpretability
    - [ ] 3.4.1 Partial Dependence Plots
    - [ ] 3.4.2 Feature Interactions
    - [ ] 3.5 Summary
 
- [ ] 4 MODEL AGNOSTIC METHODS – LOCAL INTERPRETABILITY
    - [ ] 4.1 Diagnostics+ AI – Breast Cancer Diagnosis
    - [ ] 4.2 Exploratory Data Analysis
    - [ ] 4.3 Deep Neural Networks
    - [ ] 4.3.1 Data Preparation
    - [ ] 4.3.2 Training and Evaluating DNNs
    - [ ] 4.4 Interpreting DNNs
    - [ ] 4.5 LIME
    - [ ] 4.6 SHAP
    - [ ] 4.7 Anchors
    - [ ] 4.8 Summary
 
- [ ] 5 SALIENCY MAPPING
    - [ ] 5.1 Diagnostics+ AI – Invasive Ductal Carcinoma Detection
    - [ ] 5.2 Exploratory Data Analysis
    - [ ] 5.3 Convolutional Neural Networks
    - [ ] 5.3.1 Data Preparation
    - [ ] 5.3.2 Training and Evaluating CNNs
    - [ ] 5.4 Interpreting CNNs
    - [ ] 5.4.1 Probability Landscape
    - [ ] 5.4.2 LIME
    - [ ] 5.4.3 Visual Attribution Methods
    - [ ] 5.5 Vanilla Backpropagation
    - [ ] 5.6 Guided Backpropagation
    - [ ] 5.7 Other Gradient-based Methods
    - [ ] 5.8 Grad-CAM and Guided Grad-CAM
    - [ ] 5.9 Which attribution method should I use?
    - [ ] 5.10 Summary

- [ ] 6 UNDERSTANDING LAYERS AND UNITS
    - [ ] 6.1 Visual Understanding
    - [ ] 6.2 Convolutional Neural Networks – A Recap
    - [ ] 6.3 Network Dissection Framework
    - [ ] 6.3.1 Concept Definition
    - [ ] 6.3.2 Network Probing
    - [ ] 6.3.3 Quantifying Alignment
    - [ ] 6.4 Interpreting Layers and Units
    - [ ] 6.4.1 Running Network Dissection
    - [ ] 6.4.2 Concept Detectors
    - [ ] 6.4.3 Concept Detectors by Training Task
    - [ ] 6.4.4 Visualizing Concept Detectors
    - [ ] 6.4.5 Limitations of Network Dissection
    - [ ] 6.5 Summary
    - [ ] 7  UNDERSTANDING SEMANTIC SIMILARITY
