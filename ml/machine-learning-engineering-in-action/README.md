# Machine Learning Engineering in Action

[book](https://www.manning.com/books/machine-learning-engineering-in-action)

TOC-wise progress

- [ ] 1 AN INTRODUCTION TO MACHINE LEARNING ENGINEERING
- [ ] 1.1 Why ML Engineering?
- [ ] 1.2 The core tenets of ML Engineering
- [ ] 1.2.1 Planning
- [ ] 1.2.2 Scoping & Research
- [ ] 1.2.3 Experimentation
- [ ] 1.2.4 Development
- [ ] 1.2.5 Deployment
- [ ] 1.2.6 Evaluation
- [ ] 1.3 The goals of ML Engineering
- [ ] 1.4 Summary
 
- [ ] 2 YOUR DATA SCIENCE COULD USE SOME ENGINEERING
- [ ] 2.1 Augmenting a complex profession with processes to increase success in project work
- [ ] 2.2 A foundation of simplicity
- [ ] 2.3 Co-opting principles of Agile software engineering
- [ ] 2.3.1 Communication and cooperation
- [ ] 2.3.2 Embracing and expecting change
- [ ] 2.4 The foundation of ML Engineering
- [ ] 2.5 Summary
 
- [ ] 3 BEFORE YOU MODEL: PLANNING AND SCOPING
- [ ] 3.1 Planning: you want me to predict what?!
- [ ] 3.1.1 Basic planning for a project
- [ ] 3.1.2 That first meeting
- [ ] 3.1.3 Plan for demos. Lots of demos.
- [ ] 3.1.4 Experimentation by solution building: wasting time for pride’s sake
- [ ] 3.2 Experimental Scoping: Setting expectations and boundaries
- [ ] 3.2.1 Experimental scoping for the ML team - Research
- [ ] 3.2.2 Experiment scoping for the ML team – Experimentation
- [ ] 3.3 Summary
 
- [ ] 4 BEFORE YOU MODEL: COMMUNICATION AND LOGISTICS OF PROJECTS
- [ ] 4.1 Communication: defining the problem
- [ ] 4.1.1 Understanding the problem
- [ ] 4.1.2 Critical discussion boundaries
- [ ] 4.2 Don’t waste our time: critical meetings with cross-functional teams
- [ ] 4.2.1 Experimental update meeting (Do we know what we’re doing here?)
- [ ] 4.2.2 SME review (prototype review / can we solve this?)
- [ ] 4.2.3 Development Progress Review(s) (is this thing actually going to work?)
- [ ] 4.2.4 MVP review (did you build what we asked for?)
- [ ] 4.2.5 Pre-prod review (We really hope we didn’t screw this up)
- [ ] 4.3 Setting limits on your experimentation
- [ ] 4.3.1 Set a time limit
- [ ] 4.3.2 Can you put this into production? Would you want to maintain it?
- [ ] 4.3.3 TDD vs RDD vs PDD vs CDD for ML projects
- [ ] 4.4 Business rules chaos
- [ ] 4.4.1 Embracing chaos by planning for it
- [ ] 4.4.2 Human in the loop design
- [ ] 4.4.3 What’s your backup plan?
- [ ] 4.5 How to talk about results
- [ ] 4.6 Summary
 
- [ ] 5 EXPERIMENTATION IN ACTION: PLANNING AND RESEARCHING AN ML PROJECT
- [ ] 5.1 Planning Experiments
- [ ] 5.1.1 Basic Research and Planning
- [ ] 5.1.2 Forget the blogs – read the API docs
- [ ] 5.1.3 Drawing straws for an internal hackathon
- [ ] 5.1.4 Level the playing field
- [ ] 5.2 Experimental prep work
- [ ] 5.2.1 Data analysis
- [ ] 5.2.2 Moving from script to reusable code
- [ ] 5.2.3 One last note on building reusable code for experimentation
- [ ] 5.3 Summary
 
- [ ] 6 EXPERIMENTATION IN ACTION: TESTING AND EVALUATING A PROJECT
- [ ] 6.1 Testing Ideas
- [ ] 6.1.1 Setting guidelines in code
- [ ] 6.1.2 Running quick forecasting tests
- [ ] 6.2 Whittling down the possibilities
- [ ] 6.2.1 Evaluating prototypes properly
- [ ] 6.2.2 Make a call on the direction we’re going in
- [ ] 6.2.3 So… what’s next?
- [ ] 6.3 Summary
 
- [ ] 7 EXPERIMENTATION IN ACTION: MOVING FROM PROTOTYPE TO MVP
- [ ] 7.1 Tuning: automating the annoying stuff
- [ ] 7.1.1 Tuning options
- [ ] 7.1.2 Hyperopt Primer
- [ ] 7.1.3 Using Hyperopt to tune a complex forecasting problem
- [ ] 7.2 Choosing the right tech for the platform and the team
- [ ] 7.2.1 Why Spark?
- [ ] 7.2.2 Handling tuning from the driver with SparkTrials
- [ ] 7.2.3 Handling tuning from the workers with a pandas_udf
- [ ] 7.2.4 New paradigms for teams – platforms and technologies
- [ ] 7.3 Summary
 
- [ ] 8 EXPERIMENTATION IN ACTION: FINALIZING AN MVP WITH MLFLOW AND RUNTIME OPTIMIZATION
- [ ] 8.1 Logging: code, metrics, and results
- [ ] 8.1.1 MLFlow tracking
- [ ] 8.1.2 Please stop printing and log your information
- [ ] 8.1.3 Version control, branch strategies, and working with others
- [ ] 8.2 Scalability and concurrency
- [ ] 8.2.1 What is concurrency?
- [ ] 8.2.2 What you can (and can’t) run asynchronously
- [ ] 8.3 Summary
 
- [ ] 9 MODULARITY FOR ML: WRITING TESTABLE AND LEGIBLE CODE
- [ ] 9.1 Monolithic scripts and why they are bad
- [ ] 9.1.1 How monoliths come into being
- [ ] 9.1.2 Walls of text
- [ ] 9.1.3 Considerations for monolithic scripts
- [ ] 9.2 Debugging walls of text
- [ ] 9.3 Modular ML code design
- [ ] 9.4 Test driven development for ML
- [ ] 9.5 Summary
 
- [ ] 10 STANDARDS OF CODING AND CREATING MAINTAINABLE ML CODE
- [ ] 10.1 ML code smells
- [ ] 10.2 Naming, structure, and code architecture
- [ ] 10.2.1 Naming conventions and structure
- [ ] 10.2.2 Trying to be too clever
- [ ] 10.2.3 Code architecture
- [ ] 10.3 Tuple unpacking and maintainable alternatives
- [ ] 10.3.1 Tuple unpacking example
- [ ] 10.3.2 A solid alternative to tuple unpacking
- [ ] 10.4 Blind to issues: eating exceptions and other bad practices
- [ ] 10.4.1 Try / catch with the precision of a shotgun
- [ ] 10.4.2 Exception handling with laser precision
- [ ] 10.4.3 Handling errors ‘the right way’
- [ ] 10.5 Use of global mutable objects
- [ ] 10.5.1 How mutability can burn you
- [ ] 10.5.2 Encapsulation to prevent mutable side effects
- [ ] 10.6 Excessively nested logic
- [ ] 10.7 Summary
 
- [ ] 11 BIG O(NO) AND HOW TO THINK ABOUT RUNTIME PERFORMANCE
- [ ] 11.1 What is Big-O, anyway?
- [ ] 11.1.1 A gentle introduction to complexity
- [ ] 11.2 Complexity by Example
- [ ] 11.2.1 O(1) – the ‘It doesn’t matter how big the data is’ algorithm
- [ ] 11.2.2 O(n) – The linear relationship algorithm
- [ ] 11.2.3 O(n^2) – An exponential relationship to the size of the collection
- [ ] 11.3 Analyzing decision tree complexity
- [ ] 11.4 General algorithmic complexity for ML
- [ ] 11.5 Summary
 
- [ ] 12 MODEL MEASUREMENT AND WHY IT’S SO IMPORTANT
- [ ] 12.1 Model attribution measurement
- [ ] 12.1.1 Measuring prediction performance
- [ ] 12.1.2 Clarification on Correlation vs. Causation
- [ ] 12.2 Leveraging AB testing for attribution calculations
- [ ] 12.2.1 AB testing 101
- [ ] 12.2.2 Evaluating continuous metrics
- [ ] 12.2.3 Alternative displays and tests
- [ ] 12.2.4 Evaluating categorical metrics
- [ ] 12.3 Drift detection
- [ ] 12.3.1 What influences drift?
- [ ] 12.4 Responding to drift
- [ ] 12.4.1 What can we do about it?
- [ ] 12.4.2 Responding to drift
- [ ] 12.5 Summary
 
- [ ] 13 ML DEVELOPMENT HUBRIS
- [ ] 13.1 Elegant Complexity vs. Over-Engineering
- [ ] 13.1.1 Light-weight scripted style (imperative)
- [ ] 13.1.2 An over-engineered mess
- [ ] 13.2 Unintentional obfuscation: could you read this if you didn’t write it?
- [ ] 13.2.1 The flavors of obfuscation
- [ ] 13.2.2 Troublesome coding habits recap
- [ ] 13.3 Premature generalization, premature optimization, and other bad ways to show how smart you are
- [ ] 13.3.1 Generalization and Frameworks: avoid them until you can’t
- [ ] 13.3.2 Optimizing too early
- [ ] 13.4 Do you really want to be the canary? Alpha testing and the dangers of the open-source coal mine
- [ ] 13.5 Technology driven development vs. solution driven development
- [ ] 13.6 Summary
- [ ]  PART 3: DEVELOPING PRODUCTION MACHINE LEARNING CODE
- [ ] 14  WRITING PRODUCTION CODE
- [ ] 15  QUALIFYING AND ACCEPTANCE TESTING
- [ ] 16  REWORKING
- [ ]  PART 4: INFERENCE AND AUTOMATION
- [ ] 17  SUPPORTING YOUR MODEL AND YOUR CODE
- [ ] 18  AUTOMATION TOOLING